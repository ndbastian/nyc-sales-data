---
title: "Methodology"
output: pdf_document
bibliography: bib/bibliography.bib
---

Building off of previous research (see: @antipov12; also @Schernthanner2016), the goal of this study is to employ the Random Forest algorithm to predict:

1) Probability of Sale. The probability that a given property in New York City will sell in a given year
2) Amount of Sale. Given that a property sells, how much is the sale value?


To accomplish this, we combine three open-source data repositories provided by New York City via [nyc.gov](nyc.gov) and [data.cityofnewyork.us](data.cityofnewyork.us). Our base modeling data set includes all building records and associated sales information from 2003-2017. 

Following the creation of the base modeling data, we create two additional data sets through feature engineering: a "zip code-features" data set and a "spatial-lag-features" data set. The primary goal of this study is to compare the predictive power of the spatial lag data set vs. the base and zip code level features.

## Data

The New York City government makes available an annual data set which describes all tax lots in the five boroughs. The Primary Land Use and Tax Lot Output data set, known as [PLUTO](https://www1.nyc.gov/site/planning/data-maps/open-data/bytes-archive.page?sorts[year]=0), contains a single record for every tax lot in the city along with a number of building and tax-related attributes such as Year Built, Assessed Value, Square Footage, number of stories, and many more. At the time of this writing, NYC has made this data set available for all years between 2002-2017, excluding 2008. For convenience, we also exclude the 2002 data set from our analysis because sales information is not available for that year. Importantly for our analysis, the latitude and longitude of the tax lots are also made available, allowing us to locate in space each building and to build geospatial features from the data. 

Ultimately, we are interested in sales transactions--both frequency, and amount. Sales transactions are also made available by the New York City government, known as [NYC Rolling Sales Data](http://www1.nyc.gov/site/finance/taxes/property-annualized-sales-update.page). At the time of this writing, sales transactions are available for the years 2003-2017. The sales transactions data contains additional data fields describing time, place, and amount of sale as well as additional building characteristics. Crucially, the sales transaction data does not include geographical coordinates, making it impossible to perform geospatial analysis without first mapping the sales data to PLUTO.

Prior to mapping to PLUTO, the sales data must first be transformed to include the proper mapping key. New York City uses a standard key of Borough-Block-Lot to identify tax lots in the data. For example, 31 West 27th Street is located in Manhattan, on block 829 and lot 16, therefore, its Borough-Block-Lot (BBL) is 1_829_16 (the 1 represents Manhattan). The sales data contains BBL's at the building level, however, the sales transactions data does not appropriately designate condos as their own BBLs. Mapping the sales data directly to the PLUTO data results in a mapping error rate of 23.1%. Therefore, the sales transactions data must first be mapped to another data source, the NYC Property Address Directory, or [PAD](https://data.cityofnewyork.us/City-Government/Property-Address-Directory/bc8t-ecyu/data)), which contains an exhaustive list of all BBLs in NYC. Once the sales data is combined with PAD, the data can be mapped to PLUTO with an error rate of 0.291%.

After the Sales Transactions data has been mapped to PAD, it can then be mapped to PLUTO. The sales data is normalized and filtered so that only BBLs with less than or equal to 1 transactions in a year occur. The final data set is an exhaustive list of all tax lots in NYC for every year between 2003-2017, whether that building was sold, for what amount, and several other additional variables. 


Only building categories of significant interest are included in the data. The following building types are included:

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

suppressPackageStartupMessages(library(tibble))

category_table <- tribble(
  ~Category, ~Description
  , "A", "ONE FAMILY DWELLINGS"
  , "B", "TWO FAMILY DWELLINGS"
  , "C", "WALK UP APARTMENTS"
  , "D", "ELEVATOR APARTMENTS"
  , "F", "FACTORY AND INDUSTRIAL BUILDINGS"
  , "G", "GARAGES AND GASOLINE STATIONS"
  , "L", "LOFT BUILDINGS"
  , "O", "OFFICES"
)
knitr::kable(category_table)
```


The data is further filtered to include only records with equal to or less than 2 buildings per tax lot. The global filtering of the data set reduces the base modeling data from 12,012,780 records down to 8,247,499.


## Feature Engineering & Data Partitioning


### Base modeling data
The base modeling data set is enhanced to include additional features. A summary table of the additional features are presented below:


```{r Table 1, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
knitr::kable(readr::read_rds("tables and figures/table1.rds"))
```

A binary variable is created to indicate whether a tax lot has a building on it (i.e., whether it is an empty plot of land or not). In addition, building types are quantified by what percent of the square footage belongs to the major property types: Commercial, Residential, Office, Retail, Garage, Storage, Factory and Other. 

Importantly, two variables are created from the Sales Prices: A price-per-square-foot figure ("Last_Sale_Price") and a total Sale Price ("Last_Sale_Price_Total"). Sale Price per Square foot eventually becomes the outcome variable in one of the predictive models, even though it is referred to as Sale Price. Further features are derived which carry forward the previous sale price of a tax lot, if there is one, through successive years. Previous Sale Price is then used to create Simple Moving Averages (SMA), Exponential Moving Averages (SMA), and percent change measurements between the moving averages. In total, 69 variables are input to the feature engineering process and 92 variables are output. The final base modeling data set is 92 variables by 8,247,499 rows. 


### Zip code modeling data

The first of the comparative modeling data sets is the Zip code modeling data. Using the base data as a starting point, several features are generated to describe characteristics of the zip code where the tax lot resides. A summary table of the Zip code level features is presented below. 

```{r Table 2, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
knitr::kable(readr::read_rds("tables and figures/table2.rds"))
```

In general, the base model data features are aggregated to a zip code level and attached to the individual observations, including SMA and EMA calculations. Additionally, a second set of features are added, denoted as "bt_only", which filter only for tax lots of the same building type. In total, the Zip code feature engineering process inputs 92 variables and outputs 122 variables. 


### Spatial Lag modeling data

Spatial lags are variables created from physically proximate observations. For example, taking the average building age from all buildings within 100 meters of the tax lot in question would be a spatial lag. Creating spatial lags presents both advantages and disadvantages in the modeling process. Spatial lags allow for much more fine-tuned measurements of a building's surrounding area. Knowing the average sale price of all buildings within 500 meters of a building can be much informative than knowing the sale prices of all buildings in the same zip code. However, building spatial lags is computationally expensive. 

To build spatial lags for all 8,247,499 observations in our modeling data, we created a spatial indexing technique that sped up the process by allowing for parallelization of the operation. Since tax lots rarely if ever move, we reduced the indexing task to 514,124 points (the number of unique tax lots in New York City). Then, for each point, we calculated and cached every other tax lot within 500 meters of that building. The result was an origin-destination relationship graph that connected each tax lot to its surrounding tax lots. 

Next, we used the spatial index to create spatial lag features. One advantage to using spatial lags is the rich number of potential features which can be created. Spatial lags can be weighted based on a distance function, i.e., physically closer observations can be given more weight. For our modeling purposes, we created two sets of features: distance weighted features (denoted with a "_dist") and simple average features (denoted with "_basic"). SMA and EMA as well as percent changes were also calculated. In total, the spatial lag feature engineering process input 92 variables and output 194 variables. A summary of the Spatial Lag features are presented below:

```{r Table 3, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
knitr::kable(readr::read_rds("tables and figures/table3.rds"))
```


## Algorithm

Previous work (@antipov12; also @Schernthanner2016) has found the Random Forest algorithm [@Breiman2001] suitable to prediction tasks invlovling real estate. While algorithms exist that may marginally outperform Random Forest in terms of predictive accuracy (such as nueral networks and functional gradient descent algorithms), Random Forest is highly scalable and paralellizable, and therefore a natural choice for comparing different feature engineering strategies (such as in this paper).  

Random Forest can be used for both classification and regression tasks. The Random Forest algorithm works by generating a large number of independent classification or regression decision trees and then employing  majority voting (for classification) or averaging (for regression) to generate predictions. Over a dataset of N rows by M predictors, a bootstrap sample of the data is chosen (n < N) as well as a subset of the predictors (m < M). Individual decision/regression trees are built on the n by m sample. Because the trees can be built independently (and not sequentially, as is the case with most functional gradient descent algorithms), the tree building process can be executed in parallel across an arbitrary number of computer cores. With a sufficiently large number of cores, the model training time can be significantly reduced. This provides a highly accurate, robust prediction model that avoids many of the drawbacks of traditional parametric techniques, such as OLS. 

The primary advantages to using Random Forest with real estate data are: 

1. Can handle an arbitrarily large number of variables while avoiding the curse of dimensionality associated with regression techniques. Increasing the number of predictors in a multiple regression can quickly lead to overfitting. 
2. Can accomodate categorical variables with many levels. Real estate data often contains information describing the location of the property, or the property itself, as one of a large set of possible choices, such as neighborhood, county, census tract, district, property type, and zoning information. Because factors need to be recoded as individual dummy variables in the model building process, factors with many levels will quickly encounter the curse of dimensionality in multiple regression techniques.
3. Appropriately handles missing data. Predictions can be made with the parts of the tree which are succesfully built, and therefore, there is no need to filter out incomplete observations or impute missing values. Since much real destate data is self reported, incomplete fields are common in the data.
4. Robust against outliers. Because of bootstrap sampling, outliers appear in individual trees less often, and therefore, are reduced in terms of importance. Real estate data, especially with regards to pricing, tends to contain large outliers. For example, the dependent variable in one of our models, Sale Price, exhibits the following characteristics:

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
knitr::kable(readr::read_rds("tables and figures/sale_price_summary_table4.rds"))
```

Note that the mean and median diverge, while the maximum is significantly higher than the third quartile. 

5. Can recognize non-linear relationships in data, which is useful when modeling spatial relationships. 
6. Is not affected by colinearity in the data. This is highly valuable as real estate data can be highly correlated. 
7. The algorithm can be parallelized and is relatively fast compared to nueral networks and functional gradient descent algorithms. 


## Model Diagnostics







